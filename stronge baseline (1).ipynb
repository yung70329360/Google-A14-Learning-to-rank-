{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51770,"status":"ok","timestamp":1658496207986,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"P8AJbs8CW7Q8","outputId":"bfcae4a3-f166-459e-94a0-95961cb42921"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":655,"status":"ok","timestamp":1658496145593,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"yCKhQvkR4cfE","outputId":"bb11357f-cad1-4293-c4e9-d796102e0d97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jul 22 13:22:24 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":35402,"status":"ok","timestamp":1658496259699,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"Eb-InukzXl9c"},"outputs":[],"source":["from zipfile import ZipFile\n","with ZipFile('/content/drive/MyDrive/GoogleA14/AI4Code (1).zip') as z:\n","  z.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypmxg1Klv9jM"},"outputs":[],"source":["# !gdown --id '1ehzzD7WAvSwd_mZxawsz9IYzGxR66oA1&export=download' --output GoogleA14.zip\n","\n","# !unzip -o GoogleA14.zip\n","\n","# !nvidia-smi"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":615,"status":"ok","timestamp":1658496281403,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"8QELlxe_VlHu"},"outputs":[],"source":["import json\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from scipy import sparse\n","from tqdm import tqdm\n","\n","pd.options.display.width = 180\n","pd.options.display.max_colwidth = 120\n","\n","data_dir = Path('/content')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1658496285128,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"vf5zBXCGiTYu"},"outputs":[],"source":["import random\n","import torch\n","import os \n","def same_seeds(seed):\n","\t  torch.manual_seed(seed)\n","\t  if torch.cuda.is_available():\n","\t\t    torch.cuda.manual_seed(seed)\n","\t\t    torch.cuda.manual_seed_all(seed)\n","\t  np.random.seed(seed)\n","\t  random.seed(seed)\n","\t  torch.backends.cudnn.benchmark = False\n","\t  torch.backends.cudnn.deterministic = True\n","\n","same_seeds(0)"]},{"cell_type":"markdown","metadata":{"id":"nt6uax0hZoUc"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225466,"status":"ok","timestamp":1658496514349,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"jShkEXDvYfv-","outputId":"7b4423a0-fecd-4af9-ae78-cabb3159c82e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Train NBs: 100%|██████████| 50000/50000 [03:06<00:00, 268.78it/s]\n"]}],"source":["import json\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from scipy import sparse\n","from tqdm import tqdm\n","import os\n","\n","NUM_LIMIT = 50000\n","def read_notebook(path):  \n","    return (\n","        pd.read_json(\n","            path,\n","            dtype={'cell_type': 'category', 'source': 'str'})\n","            .assign(id=path.stem)\n","            .rename_axis('cell_id')\n","    )\n","\n","def get_ranks(base, derived):\n","    assert type(base)==list, print(base,'/n', derived)\n","    return [base.index(d) for d in derived]\n","\n","if NUM_LIMIT == None:\n","  try :\n","    df = pd.read_csv('/content/drive/MyDrive/GoogleA14/df.csv')\n","\n","    df_orders = pd.read_csv(\n","      data_dir / 'train_orders.csv',\n","      index_col='id',\n","      squeeze=True,\n","    ).str.split()  # Split the string representation of cell_ids into a list\n","\n","    df_orders_ = df_orders.to_frame().join(\n","        df.groupby('id')['cell_id'].apply(list),\n","        how='right',\n","    ).dropna()\n","  except:\n","    paths_train = list((data_dir / 'train').glob('*.json'))\n","    notebooks_train = [\n","      read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n","    ]\n","    df = (\n","        pd.concat(notebooks_train)\n","            .set_index('id', append=True)\n","            .swaplevel()\n","            .sort_index(level='id', sort_remaining=False)\n","    )\n","\n","    df.to_csv('/content/drive/MyDrive/GoogleA14/df.csv')\n","\n","    df_orders = pd.read_csv(\n","        data_dir / 'train_orders.csv',\n","        index_col='id',\n","        squeeze=True,\n","    ).str.split()  # Split the string representation of cell_ids into a list\n","\n","    df_orders_ = df_orders.to_frame().join(\n","        df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n","        how='right',\n",")\n","else:\n","  paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_LIMIT]\n","  notebooks_train = [\n","       read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')              \n","  ]\n","  df = (\n","        pd.concat(notebooks_train)\n","            .set_index('id', append=True)\n","            .swaplevel()\n","            .sort_index(level='id', sort_remaining=False)\n","    )\n","  df_orders = pd.read_csv(\n","      data_dir / 'train_orders.csv',\n","      index_col='id',\n","      squeeze=True,\n","  ).str.split()  # Split the string representation of cell_ids into a list\n","\n","  df_orders_ = df_orders.to_frame().join(\n","      df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n","      how='right',\n",")\n","\n","\n","ranks = {}\n","for id_, cell_order, cell_id in df_orders_.itertuples():\n","    try:\n","      rank = get_ranks(cell_order, cell_id)\n","    except:\n","      print('cell_order:', cell_order, type(cell_order))\n","      print('cell_id:', cell_id, type(cell_id))\n","      get_ranks(cell_order, cell_id)\n","    ranks[id_] = {'cell_id': cell_id, 'rank': rank }\n","df_ranks = (\n","    pd.DataFrame\n","        .from_dict(ranks, orient='index')\n","        .rename_axis('id')\n","        .apply(pd.Series.explode)\n","        .set_index('cell_id', append=True)\n",")\n","\n","df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n","df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\n","df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n","\n","from sklearn.model_selection import GroupShuffleSplit\n","\n","NVALID = 0.1  # size of validation set\n","splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n","train_ind, val_ind = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n","train_df = df.loc[train_ind].reset_index(drop=True)\n","val_df = df.loc[val_ind].reset_index(drop=True)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213222,"status":"ok","timestamp":1658496727562,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"4KzHA6id4ad4","outputId":"54e31aab-1eb5-48f0-a60c-1f7776e9e07a"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","100%|██████████| 5017/5017 [00:19<00:00, 253.73it/s]\n","100%|██████████| 44983/44983 [02:37<00:00, 284.74it/s]\n"]}],"source":["# Base markdown dataframes\n","try:\n","  os.makedirs('/content/data')\n","except:\n","  pass\n","train_df_mark = train_df[train_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)\n","val_df_mark = val_df[val_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True)\n","train_df_mark.to_csv(\"/content/data/train_mark.csv\", index=False)\n","val_df_mark.to_csv(\"/content/data/val_mark.csv\", index=False)\n","val_df.to_csv(\"/content/data/val.csv\", index=False)\n","train_df.to_csv(\"/content/data/train.csv\", index=False)\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","stemmer = WordNetLemmatizer()\n","\n","# Additional code cells\n","def clean_code(cell):\n","    str(cell).replace(\"\\\\n\", \"\\n\")\n","    try:\n","      tokens = cell.split()\n","    except:\n","      print(cell)\n","    \n","    tokens = [stemmer.lemmatize(word) for word in tokens]\n","    preprocessed_text = ' '.join(tokens)\n","    return preprocessed_text\n","\n","\n","\n","def sample_cells(cells, n):\n","    cells = [clean_code(cell) for cell in cells]\n","    if n >= len(cells):\n","        return [cell[:200] for cell in cells]      ####################\n","    else:\n","        results = []\n","        step = len(cells) / n\n","        idx = 0\n","        while int(np.round(idx)) < len(cells):\n","            results.append(cells[int(np.round(idx))])\n","            idx += step\n","        assert cells[0] in results\n","        if cells[-1] not in results:\n","            results[-1] = cells[-1]\n","        return results\n","\n","\n","def get_features(df):\n","    features = dict()\n","    df = df.sort_values(\"rank\").reset_index(drop=True)\n","    for idx, sub_df in tqdm(df.groupby(\"id\")):\n","        features[idx] = dict()\n","        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n","        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n","        total_code = code_sub_df.shape[0]\n","        codes = sample_cells(code_sub_df.source.values, 20)\n","        features[idx][\"total_code\"] = total_code\n","        features[idx][\"total_md\"] = total_md\n","        features[idx][\"codes\"] = codes\n","    return features\n","\n","val_fts = get_features(val_df)\n","json.dump(val_fts, open(\"/content/data/val_fts.json\",\"wt\"))\n","train_fts = get_features(train_df)\n","json.dump(train_fts, open(\"/content/data/train_fts.json\",\"wt\"))"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1106,"status":"ok","timestamp":1658496728656,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"cNWXwdZYcHja"},"outputs":[],"source":["train_df_order = train_df.sort_values(by = ['id', 'pct_rank'])"]},{"cell_type":"markdown","metadata":{"id":"24KjafwIhnNR"},"source":["# Dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11357,"status":"ok","timestamp":1658496740010,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"mNrJQrzZhPgt","outputId":"5a1dd0e9-acf0-485d-d98b-0dc4b65cab0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 32.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 90.3 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 85.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}],"source":["from torch.utils.data import DataLoader, Dataset\n","import torch\n","try:\n","  from transformers import AutoTokenizer\n","except:\n","  !pip install transformers\n","  from transformers import AutoTokenizer\n","\n","class MarkdownDataset(Dataset):\n","\n","    def __init__(self, df, model_name_or_path, total_max_len, md_max_len, fts):\n","        super().__init__()\n","        self.df = df.reset_index(drop=True)\n","        self.md_max_len = md_max_len\n","        self.total_max_len = total_max_len  # maxlen allowed by model config\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=True)\n","        self.fts = fts\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        md = row.cell_id\n","\n","        inputs = self.tokenizer.encode_plus(\n","            row.source,\n","            None,\n","            add_special_tokens=False,\n","            max_length=self.md_max_len,\n","            padding=False,\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        code_inputs = self.tokenizer.batch_encode_plus(\n","            [str(x) for x in self.fts[row.id][\"codes\"]],\n","            add_special_tokens=False,\n","            max_length=23,\n","            padding=False,\n","            truncation=True\n","        )\n","        n_md = self.fts[row.id][\"total_md\"]\n","        n_code = self.fts[row.id][\"total_code\"]\n","        if n_md + n_code == 0:\n","            fts = torch.FloatTensor([0])\n","        else:\n","            fts = torch.FloatTensor([n_md / (n_md + n_code)])\n","\n","        ids = [101] + inputs['input_ids'] + [102]\n","        md_len = len(ids)\n","        for x in code_inputs['input_ids']:\n","            ids.extend(x)\n","            ids.extend([102])    #############\n","            \n","        ids = ids[:self.total_max_len]\n","        if len(ids) != self.total_max_len:\n","          ids, mask = self.padding(md_len, ids)\n","        else:\n","          mask = torch.LongTensor([1]*len(ids))\n","          ids = torch.LongTensor(ids)\n","        \n","        assert len(ids) == self.total_max_len, print('len(ids) is not same to total_max_len')\n","        assert len(ids) == len(mask), print('len(ids):', len(ids), 'len(mask):', len(mask))\n","\n","        return ids, mask, fts,  torch.FloatTensor([row.pct_rank]), row.id, md\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","    \n","    def padding(self, md_len, ids):\n","      all_len = len(ids)\n","      ids_ = ids + [1]*(self.total_max_len - all_len)\n","      mask = [1]*all_len + [0]*(self.total_max_len - all_len)\n","      return torch.LongTensor(ids_), torch.LongTensor(mask) "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1658496740010,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"IKnMmeGzn-xw"},"outputs":[],"source":["model_name_or_path = '/content/drive/MyDrive/GoogleA14/codebert-base'\n","train_mark_path = '/content/data/train_mark.csv'\n","train_features_path = '/content/data/train_fts.json'\n","val_mark_path = '/content/data/val_mark.csv'\n","val_features_path = '/content/data/val_fts.json'\n","val_path = \"/content/data/val.csv\"\n","\n","val_steps =100 \n","md_max_len = 64\n","total_max_len = 512\n","batch_size = 8\n","accumulation_steps = 4\n","n_workers = 4\n","early_stop_step = 100"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12911,"status":"ok","timestamp":1658496752909,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"6I0L6yC9n6oI"},"outputs":[],"source":["import json\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","import sys, os\n","import torch\n","\n","\n","\n","try:\n","  os.mkdir(\"/content/outputs\")\n","except:\n","  pass\n","\n","train_df_mark = pd.read_csv(train_mark_path).drop(\"parent_id\", axis=1).dropna().reset_index(drop=True)\n","train_fts = json.load(open(train_features_path))\n","val_df_mark = pd.read_csv(val_mark_path).drop(\"parent_id\", axis=1).dropna().reset_index(drop=True)\n","val_fts = json.load(open(val_features_path))\n","val_df = pd.read_csv(val_path)\n","\n","order_df = pd.read_csv(\"/content/train_orders.csv\").set_index(\"id\")\n","df_orders = pd.read_csv(\n","    data_dir / 'train_orders.csv',\n","    index_col='id',\n","    squeeze=True,\n",").str.split()\n","\n","train_ds = MarkdownDataset(train_df_mark, model_name_or_path=model_name_or_path, md_max_len=md_max_len,\n","                           total_max_len=total_max_len, fts=train_fts)\n","val_ds = MarkdownDataset(val_df_mark, model_name_or_path=model_name_or_path, md_max_len=md_max_len,\n","                         total_max_len=total_max_len, fts=val_fts)\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=n_workers,\n","                          pin_memory=False, drop_last=True)\n","val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=n_workers,\n","                        pin_memory=False, drop_last=False)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1658424278227,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"9Ex8tX_zoh_T","outputId":"d96da873-cb1c-46d7-8ada-b9750ff64e30"},"outputs":[{"name":"stdout","output_type":"stream","text":["8\n"]}],"source":["for i in train_loader:\n","  print(len(i[0]))\n","  break"]},{"cell_type":"markdown","metadata":{"id":"y39jX7UgicNx"},"source":["# Model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2234,"status":"ok","timestamp":1658496755133,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"f_7piTDviD6y"},"outputs":[],"source":["import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n","\n","\n","class MarkdownModel(nn.Module):\n","    def __init__(self, model_path):\n","        super(MarkdownModel, self).__init__()\n","        self.model = AutoModel.from_pretrained(model_path)\n","        # for name, param in self.model.named_parameters():\n","        #     param.requires_grad = False # unfreeze weights in at all\n","        #     if param.requires_grad == False:\n","        #       print('Freeze', name)\n","        #     else:\n","        #       print('Fail Freezing')\n","        \n","        self.top = nn.Linear(769, 1)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, ids, mask, fts):\n","        x = self.model(ids, mask)[0]\n","        x = torch.cat((x[:, 0, :], fts), 1)\n","        x = self.dropout(x)\n","        x = self.top(x)\n","        x = torch.sigmoid(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"hh87wNNKij4h"},"source":["# Metrics\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658496755134,"user":{"displayName":"Yung H","userId":"17498145056017003632"},"user_tz":-480},"id":"KqzHULV9ieI5"},"outputs":[],"source":["from bisect import bisect\n","\n","def count_inversions(a):\n","    inversions = 0\n","    sorted_so_far = []\n","    for i, u in enumerate(a):\n","        j = bisect(sorted_so_far, u)\n","        inversions += i - j\n","        sorted_so_far.insert(j, u)\n","    return inversions\n","\n","\n","def kendall_tau(ground_truth, predictions):\n","    total_inversions = 0\n","    total_2max = 0  # twice the maximum possible inversions across all instances\n","    for gt, pred in zip(ground_truth, predictions):\n","        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n","        total_inversions += count_inversions(ranks)\n","        n = len(gt)\n","        total_2max += n * (n - 1)\n","    return 1 - 4 * total_inversions / total_2max"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioyp99UkhqhR"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"ztyhMiTJizaT"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc7VV_GebGVL"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Ie69KgCi5PN","outputId":"58609a28-dff2-4fb7-8765-6315a7a6fc24"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 Loss: 0.03856188431382179 lr: [2.2268076981419192e-05, 2.2268076981419192e-05]:  29%|██▉       | 25778/87437 [2:12:10<5:16:29,  3.25it/s]"]}],"source":["def read_data(data):\n","    return tuple(d.cuda() for d in data[:-3]), data[-3].cuda(), data[-2], data[-1]\n","\n","def correct_place_or_not(preds, id_list, md_list):\n","  for pred, id, md in zip(preds, id_list, md_list):\n","    print('pred', pred)\n","    print('id', id)\n","    print('md', md)\n","\n","    id_df = train_df_order[train_df_order['id'] == id]\n","    md_rank = id_df[id_df['cell_id'] == md].rank\n","    md_pct = id_df[id_df['cell_id'] == md].pct_rank\n","    correct_pct = id_df[id_df['cell_id'] == md].pct_rank\n","    \n","\n","    if md_rank != 0:\n","      previous_pct = id_df[id_df['rank'] == (md_rank-1)].pct_rank\n","    if md_rank != (len(id_df) - 1):\n","      follow_pct = id_df[id_df['rank'] == (md_rank+1)].pct_rank\n","    if previous_pct:\n","      if md_pct < previous_pct:\n","        loss = previous_pct - md_pct\n","        return loss\n","    elif follow_pct:\n","      if md_pct > follow_pct:\n","        loss = md_pct - follow_pct\n","        return loss\n","    else:\n","      return 0\n","\n","  \n","\n","def validate(model, val_loader):\n","    model.eval()\n","\n","    tbar = tqdm(val_loader, file=sys.stdout)\n","\n","    preds = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for idx, data in enumerate(tbar):\n","            inputs, target, id_list, md_list  = read_data(data)\n","\n","            with torch.cuda.amp.autocast():\n","                pred = model(*inputs)\n","\n","            preds.append(pred.detach().cpu().numpy().ravel())\n","            labels.append(target.detach().cpu().numpy().ravel())\n","    model.train()\n","    return np.concatenate(labels), np.concatenate(preds)\n","\n","\n","def train(model, train_loader, val_loader, epochs):\n","    np.random.seed(0)\n","    # Creating optimizer and lr schedulers\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ] ######################################\n","\n","    num_train_optimization_steps = int(epochs * len(train_loader) / accumulation_steps)\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5,\n","                      correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n","                                                num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","\n","    criterion = torch.nn.L1Loss()\n","    scaler = torch.cuda.amp.GradScaler()\n","    scores = []\n","    max_score = 0 \n","    early_stop = 0 \n","    for e in range(epochs):\n","        model.train()\n","        tbar = tqdm(train_loader, file=sys.stdout)\n","        loss_list = []\n","        preds = []\n","        labels = []\n","\n","        for idx, data in enumerate(tbar):\n","            inputs, target, id_list, md_list = read_data(data)\n","\n","            \n","            with torch.cuda.amp.autocast():\n","                pred = model(*inputs)\n","                loss = criterion(pred, target)\n","            \n","            #pct_loss = correct_place_or_not(pred, id_list, md_list)\n","            # loss = (loss+pct_loss) / accumulation_steps\n","\n","            loss = loss / accumulation_steps\n","\n","            \n","            scaler.scale(loss).backward()\n","            if idx % accumulation_steps == 0 or idx == len(tbar) - 1:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                scheduler.step()\n","\n","            loss_list.append(loss.detach().cpu().item())\n","            preds.append(pred.detach().cpu().numpy().ravel())\n","            labels.append(target.detach().cpu().numpy().ravel())\n","\n","            # if idx & val_steps ==0 or  idx == len(tbar) - 1:\n","            #   y_val, y_pred = validate(model, val_loader)\n","            #   val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n","            #   val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n","            #   y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n","            #   score = kendall_tau(df_orders.loc[y_dummy.index], y_dummy)\n","            #   scores.append(score)\n","            #   if score > min_score:\n","            #     max_score = score\n","            #     torch.save(model.state_dict(), '/content/drive/MyDrive/GoogleA14/listwise_codebert.bin')\n","            #     print(\"Preds score\", score)\n","            #     early_stop_step = 0\n","            #   else:\n","            #     if early_stop == early_stop_step:\n","            #       break \n","            #     else:\n","            #       early_stop_step += 1\n","\n","\n","            tbar.set_description(f\"Epoch {e + 1} Loss: {loss} lr: {scheduler.get_last_lr()}\")\n","\n","        y_val, y_pred = validate(model, val_loader)\n","        val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n","        val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n","        y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n","        score = kendall_tau(df_orders.loc[y_dummy.index], y_dummy)\n","        print(\"Final Preds score\", score)\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/GoogleA14/listwise_codebert.bin')\n","        \n","        # scores.append(score)\n","        # if score > min_score:\n","        #    max_score = score\n","        #    torch.save(model.state_dict(), '/content/drive/MyDrive/GoogleA14/listwise_codebert.bin')\n","\n","    return model, y_pred ,scores\n","same_seeds(0)\n","model = MarkdownModel(model_name_or_path)\n","model = model.cuda()\n","model, y_pred , scores= train(model, train_loader, val_loader, epochs=1)\n"," "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"fBTkjfInQf1R","executionInfo":{"status":"ok","timestamp":1658495751724,"user_tz":-480,"elapsed":2,"user":{"displayName":"Yung H","userId":"17498145056017003632"}}},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"stronge baseline.ipynb","provenance":[],"authorship_tag":"ABX9TyNUkNaCIBeuPPKZZvG4LIg0"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}